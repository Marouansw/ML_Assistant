{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(r\"C:\\Users\\LENOVO\\Desktop\\ML_RAG\\RAG\\Structured_Data.pdf\")\n",
    "data = loader.load()\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Churn des clients ( télécommunication)\n",
      "Ce use case a pour objectif de prédire les clients susceptibles de partir afin d’anticiper la\n",
      "majorité des départs des clients et ainsi le département marketing peut mener des actions\n",
      "sur cette population pour la fidéliser\n",
      "* Identifiant de la ligne (numéro de téléphone)\n",
      "* Ancienneté de la ligne en jours (date courante - date d'activation de la ligne)\n",
      "* Date d'activation de la ligne\n",
      "* Date de résiliation de la ligne\n",
      "* Age du client (en année)\n",
      "* Catégorie socio-professionnelle\n",
      "* Ville\n",
      "* Sexe\n",
      "* Etat matrimonial\n",
      "* Type identification (registre du commerce)\n",
      "* Rapport entre la durée des appels sortants et la durée des appels entrant durant les 6 derniers\n",
      "mois\n",
      "* Durée totale des appels (Entrant + Sortant) durant les 6 derniers mois\n",
      "* Part des appels (Entrant + Sortant) réalisés le week-end parmi la totalité des appels (Entrant +\n",
      "Sortant)\n",
      "* Part des appels (Entrant + Sortant) réalisés le soir (20h à 8h) parmi la totalité des appels\n",
      "(Entrant + Sortant)\n",
      "* Nombre total de SMS internationaux envoyés durant les 6 derniers mois\n",
      "* Nombre total de SMS nationaux envoyés vers mobiles MEDITEL durant les 6 derniers mois\n",
      "* Durée des appels sortants réalisés le week-end\n",
      "* Durée moyenne des appels vocaux sortants lors du mois M-1, M-2, M-3, M-4, M-5, M-6\n",
      "* Nombre et durée des appels sortants lors du mois M-1, M-2, M-3, M-4, M-5, M-6\n",
      "* Nombre et durée des appels nationaux, et internationaux, entrants et sortants lors du\n",
      "mois M-1,M-2,M-3,M-4,M-5, M-6\n",
      "* Plainte la plus fréquente lors des 6 derniers mois (Si plusieurs plaintes ont la même\n",
      "* Nombre total de plaintes durant le dernier mois\n",
      "* Nombre de jours écoulés depuis la dernière plainte\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=300\n",
    ")\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "print((chunks[0].page_content))\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\Desktop\\ML_RAG\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "import os\n",
    "\n",
    "\n",
    "model_name = \"Lajavaness/sentence-camembert-large\"  \n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=data, embedding=embeddings_model)\n",
    "retriever = vector_store.as_retriever()\n",
    "# return retriever\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groq \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key_here\"\n",
    "\n",
    "groq_llm = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "groq_llm1 = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothetical Document Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define the prompt template\n",
    "def hypothetical_document_generation(query) :\n",
    "   \n",
    "    prompt = PromptTemplate.from_template(\"\"\"Vous etes expert en machine learning, Citer quelques variables necessaires que doit un jeu de donnee contenenir pour resoudre \n",
    "    le probleme machine learning de la question suivante : {input}\n",
    "    ** Decris d'abord le probleme machine learning en deux phrases max .\n",
    "    Reponse: en français\"\"\")\n",
    "\n",
    "# LLMChain is a chain that takes a prompt and an LLM and runs them together\n",
    "    llm_chain = LLMChain(llm=groq_llm, prompt=prompt)\n",
    "\n",
    "\n",
    "# generating the hypothetical document \n",
    "    response = llm_chain.run(input=query)\n",
    "    return response\n",
    "\n",
    "# Get relevent documents using the hypothetical document instead of the original query to increase the performance of the retriever\n",
    "\n",
    "# hypothetical_doc = hypothetical_document_generation(\"Comment predire les transactions frauduleuses\")\n",
    "# print(hypothetical_doc)\n",
    "# retrieved_docs = retriever.get_relevant_documents(hypothetical_doc, k=1)\n",
    "# print(retrieved_docs[0].page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                                  Version\n",
      "---------------------------------------- -----------\n",
      "aiohappyeyeballs                         2.6.1\n",
      "aiohttp                                  3.11.14\n",
      "aiosignal                                1.3.2\n",
      "annotated-types                          0.7.0\n",
      "anyio                                    4.9.0\n",
      "asgiref                                  3.8.1\n",
      "asttokens                                3.0.0\n",
      "attrs                                    25.3.0\n",
      "backoff                                  2.2.1\n",
      "bcrypt                                   4.3.0\n",
      "build                                    1.2.2.post1\n",
      "cachetools                               5.5.2\n",
      "certifi                                  2025.1.31\n",
      "charset-normalizer                       3.4.1\n",
      "chromadb                                 1.0.10\n",
      "click                                    8.1.8\n",
      "colorama                                 0.4.6\n",
      "coloredlogs                              15.0.1\n",
      "comm                                     0.2.2\n",
      "dataclasses-json                         0.6.7\n",
      "debugpy                                  1.8.13\n",
      "decorator                                5.2.1\n",
      "Deprecated                               1.2.18\n",
      "distro                                   1.9.0\n",
      "duckduckgo_search                        8.0.2\n",
      "durationpy                               0.10\n",
      "executing                                2.2.0\n",
      "faiss-cpu                                1.10.0\n",
      "fastapi                                  0.115.9\n",
      "filelock                                 3.18.0\n",
      "filetype                                 1.2.0\n",
      "flatbuffers                              25.2.10\n",
      "frozenlist                               1.5.0\n",
      "fsspec                                   2025.3.2\n",
      "google-auth                              2.40.2\n",
      "googleapis-common-protos                 1.70.0\n",
      "greenlet                                 3.1.1\n",
      "groq                                     0.25.0\n",
      "grpcio                                   1.71.0\n",
      "h11                                      0.14.0\n",
      "httpcore                                 1.0.7\n",
      "httptools                                0.6.4\n",
      "httpx                                    0.28.1\n",
      "httpx-sse                                0.4.0\n",
      "huggingface-hub                          0.30.2\n",
      "humanfriendly                            10.0\n",
      "idna                                     3.10\n",
      "importlib_metadata                       8.6.1\n",
      "importlib_resources                      6.5.2\n",
      "ipykernel                                6.29.5\n",
      "ipython                                  9.0.2\n",
      "ipython_pygments_lexers                  1.1.1\n",
      "jedi                                     0.19.2\n",
      "Jinja2                                   3.1.6\n",
      "joblib                                   1.4.2\n",
      "jsonlines                                4.0.0\n",
      "jsonpatch                                1.33\n",
      "jsonpointer                              3.0.0\n",
      "jsonschema                               4.23.0\n",
      "jsonschema-specifications                2025.4.1\n",
      "jupyter_client                           8.6.3\n",
      "jupyter_core                             5.7.2\n",
      "kubernetes                               32.0.1\n",
      "langchain                                0.3.21\n",
      "langchain-community                      0.3.20\n",
      "langchain-core                           0.3.52\n",
      "langchain-groq                           0.3.2\n",
      "langchain-huggingface                    0.1.2\n",
      "langchain-mistralai                      0.2.10\n",
      "langchain-nomic                          0.1.4\n",
      "langchain-ollama                         0.3.0\n",
      "langchain-text-splitters                 0.3.7\n",
      "langgraph                                0.4.8\n",
      "langgraph-checkpoint                     2.0.26\n",
      "langgraph-prebuilt                       0.2.2\n",
      "langgraph-sdk                            0.1.70\n",
      "langsmith                                0.3.18\n",
      "loguru                                   0.7.3\n",
      "lxml                                     5.4.0\n",
      "markdown-it-py                           3.0.0\n",
      "MarkupSafe                               3.0.2\n",
      "marshmallow                              3.26.1\n",
      "matplotlib-inline                        0.1.7\n",
      "mdurl                                    0.1.2\n",
      "mmh3                                     5.1.0\n",
      "mpmath                                   1.3.0\n",
      "multidict                                6.2.0\n",
      "mypy-extensions                          1.0.0\n",
      "nest-asyncio                             1.6.0\n",
      "networkx                                 3.4.2\n",
      "nomic                                    3.4.1\n",
      "numpy                                    2.2.4\n",
      "oauthlib                                 3.2.2\n",
      "ollama                                   0.4.7\n",
      "onnxruntime                              1.22.0\n",
      "opentelemetry-api                        1.33.1\n",
      "opentelemetry-exporter-otlp-proto-common 1.33.1\n",
      "opentelemetry-exporter-otlp-proto-grpc   1.33.1\n",
      "opentelemetry-instrumentation            0.54b1\n",
      "opentelemetry-instrumentation-asgi       0.54b1\n",
      "opentelemetry-instrumentation-fastapi    0.54b1\n",
      "opentelemetry-proto                      1.33.1\n",
      "opentelemetry-sdk                        1.33.1\n",
      "opentelemetry-semantic-conventions       0.54b1\n",
      "opentelemetry-util-http                  0.54b1\n",
      "orjson                                   3.10.16\n",
      "ormsgpack                                1.10.0\n",
      "overrides                                7.7.0\n",
      "packaging                                24.2\n",
      "pandas                                   2.2.3\n",
      "parso                                    0.8.4\n",
      "pillow                                   10.4.0\n",
      "pip                                      23.2.1\n",
      "platformdirs                             4.3.7\n",
      "posthog                                  4.2.0\n",
      "primp                                    0.15.0\n",
      "prompt_toolkit                           3.0.50\n",
      "propcache                                0.3.1\n",
      "protobuf                                 6.31.1\n",
      "psutil                                   7.0.0\n",
      "pure_eval                                0.2.3\n",
      "pyarrow                                  19.0.1\n",
      "pyasn1                                   0.6.1\n",
      "pyasn1_modules                           0.4.2\n",
      "pydantic                                 2.10.6\n",
      "pydantic_core                            2.27.2\n",
      "pydantic-settings                        2.8.1\n",
      "Pygments                                 2.19.1\n",
      "PyJWT                                    2.10.1\n",
      "pypdf                                    5.4.0\n",
      "PyPika                                   0.48.9\n",
      "pyproject_hooks                          1.2.0\n",
      "pyreadline3                              3.5.4\n",
      "python-dateutil                          2.9.0.post0\n",
      "python-dotenv                            1.1.0\n",
      "pytz                                     2025.2\n",
      "pywin32                                  310\n",
      "PyYAML                                   6.0.2\n",
      "pyzmq                                    26.3.0\n",
      "referencing                              0.36.2\n",
      "regex                                    2024.11.6\n",
      "requests                                 2.32.3\n",
      "requests-oauthlib                        2.0.0\n",
      "requests-toolbelt                        1.0.0\n",
      "rich                                     13.9.4\n",
      "rpds-py                                  0.25.1\n",
      "rsa                                      4.9.1\n",
      "safetensors                              0.5.3\n",
      "scikit-learn                             1.6.1\n",
      "scipy                                    1.15.2\n",
      "sentence-transformers                    4.1.0\n",
      "setuptools                               79.0.0\n",
      "shellingham                              1.5.4\n",
      "six                                      1.17.0\n",
      "sniffio                                  1.3.1\n",
      "SQLAlchemy                               2.0.39\n",
      "stack-data                               0.6.3\n",
      "starlette                                0.45.3\n",
      "sympy                                    1.13.1\n",
      "tenacity                                 9.0.0\n",
      "threadpoolctl                            3.6.0\n",
      "tiktoken                                 0.9.0\n",
      "tokenizers                               0.21.1\n",
      "torch                                    2.6.0\n",
      "tornado                                  6.4.2\n",
      "tqdm                                     4.67.1\n",
      "traitlets                                5.14.3\n",
      "transformers                             4.51.3\n",
      "typer                                    0.15.4\n",
      "typing_extensions                        4.12.2\n",
      "typing-inspect                           0.9.0\n",
      "typing-inspection                        0.4.1\n",
      "tzdata                                   2025.2\n",
      "urllib3                                  2.3.0\n",
      "uvicorn                                  0.34.2\n",
      "watchfiles                               1.0.5\n",
      "wcwidth                                  0.2.13\n",
      "websocket-client                         1.8.0\n",
      "websockets                               15.0.1\n",
      "win32_setctime                           1.2.0\n",
      "wrapt                                    1.17.2\n",
      "xxhash                                   3.5.0\n",
      "yarl                                     1.18.3\n",
      "zipp                                     3.21.0\n",
      "zstandard                                0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve relevent documets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_documents(hypothetical_document,original_query):\n",
    "# Get relevent documents using the hypothetical document instead of the original query to increase the performance of the retriever\n",
    "    retrieved_docs_1 = retriever.invoke(hypothetical_document,k=3)\n",
    "    # print(retrieved_docs_1[0].page_content)\n",
    "# Get relevent documents using the original query\n",
    "    retrieved_docs_2 = retriever.invoke(original_query,k=3)\n",
    "    # print(retrieved_docs_2[0].page_content)\n",
    "# Combine the two sets of documents     \n",
    "    combined_docs = retrieved_docs_1 + retrieved_docs_2\n",
    "    # print(len(combined_docs))\n",
    "# Remove duplicates based on the page content\n",
    "    unique_docs = {doc.page_content: doc for doc in combined_docs}.values()\n",
    "    return list(unique_docs) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain pre-defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'required'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m hyde = HypotheticalDocumentEmbedder(llm_chain=llm_chain, base_embeddings=embeddings_model)\n\u001b[32m     12\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mComment predire le risque de credit au secteur bancaie ?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m vector = \u001b[43mhyde\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\Desktop\\ML_RAG\\venv\\Lib\\site-packages\\langchain\\chains\\hyde\\base.py:78\u001b[39m, in \u001b[36mHypotheticalDocumentEmbedder.embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m     77\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate a hypothetical document and embedded it.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     var_name = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_keys\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     79\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.llm_chain.invoke({var_name: text})\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_chain, LLMChain):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\Desktop\\ML_RAG\\venv\\Lib\\site-packages\\langchain\\chains\\hyde\\base.py:43\u001b[39m, in \u001b[36mHypotheticalDocumentEmbedder.input_keys\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minput_keys\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m     42\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Input keys for Hyde's LLM chain.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_schema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrequired\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'required'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import HypotheticalDocumentEmbedder\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Use a simple prompt for hyde\n",
    "hyde_prompt = PromptTemplate.from_template(\n",
    "\t\"Générez un document hypothétique qui répond à la question suivante : {question}\"\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=groq_llm, prompt=hyde_prompt)\n",
    "hyde = HypotheticalDocumentEmbedder(llm_chain=llm_chain, base_embeddings=embeddings_model)\n",
    "\n",
    "query = \"Comment predire le risque de credit au secteur bancaie ?\"\n",
    "vector = hyde.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m.llm_chain.input_schema)\n",
      "\u001b[31mNameError\u001b[39m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "print(self.llm_chain.input_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        SystemMessage(content=\"\"\"Tu es un assistant expert en traitement de texte et en compréhension de documents techniques.\n",
    "\n",
    "Ta tâche est d’évaluer la **pertinence d’un document** par rapport à une **question en machine learning**.\n",
    "\n",
    "### Instructions :\n",
    "\n",
    "1. Lis attentivement la **question utilisateur**.\n",
    "2. Lis ensuite le **document** fourni.\n",
    "3. Attribue un **score de pertinence** entre **00** (pas du tout pertinent) et **100** (parfaitement pertinent) selon les critères suivants :\n",
    "   - Le document répond-il à la question ou aide-t-il à y répondre ?\n",
    "   - Contient-il des informations directement utiles (données, variables, structure, contexte, logique...) ?\n",
    "   - Est-ce qu’il permet d’élaborer une solution ou une stratégie machine learning basée sur la question ?\n",
    "4. Ne repete pas le meme score pour differents documents liee a la meme question, Voir bien la difference et donner un score different pour chaque dpcument\n",
    "   ** Ranking depuis 90 a 100 est pour le plus pertinent\n",
    "\n",
    "- Repondez **uniquement** par le score \n",
    "\n",
    "---\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", return_messages=True,k=4)\n",
    "\n",
    "legacy_chain = LLMChain(\n",
    "    llm=groq_llm1,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_retrieved_docs(retrieved_docs, query):\n",
    "    \"\"\"\n",
    "    Clean the retrieved documents based on their relevance to the query.\n",
    "    \"\"\" \n",
    "    crd = {}\n",
    "\n",
    "    for doc in retrieved_docs:\n",
    "    # Use the LLMChain to get the relevance score\n",
    "        user_input = f\"Document : {doc.page_content} \\n Question : {query}\"\n",
    "\n",
    "        # relevance_score = conversation_buf.predict(question=query,document=doc.page_content)\n",
    "        relevance_score = legacy_chain.invoke({\"text\": user_input})\n",
    "        # print(f\"Document: {doc.page_content}\")\n",
    "        # print(f\"Relevance outcome : {relevance_score}\")\n",
    "        score = int(relevance_score[\"text\"])\n",
    "        # score = int(relevance_score.content)\n",
    "        print(score,type(score))\n",
    "        if score >= 90 :\n",
    "            crd[doc.page_content] = score\n",
    "    if crd:\n",
    "    # Sort the dictionary by value in descending order and take the top 2\n",
    "        most_relevant = sorted(crd, key=crd.get, reverse=True)[:2]\n",
    "        return most_relevant\n",
    "    else :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_report (query,context) :  \n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "Tu es un expert en machine learning. Tu vas recevoir :\n",
    "\n",
    "- Une **question utilisateur** qui décrit une tâche ou un problème à résoudre avec le machine learning.\n",
    "- Un **contexte** qui contient les informations sur le dataset disponible (features, types, cible, etc.).\n",
    "\n",
    "Ta tâche est de rédiger un **cahier des charges complet** qui décrit l’approche machine learning à adopter pour répondre à la question, en t’appuyant exclusivement sur le dataset fourni dans le contexte.\n",
    "\n",
    "Le cahier des charges doit inclure :\n",
    "- Une reformulation du problème et son objectif.\n",
    "- Une description du dataset (variables, types, cible, problèmes éventuels).\n",
    "- Le type de problème (classification, régression...).\n",
    "- Les étapes de préparation des données.\n",
    "- Le ou les modèles recommandés, avec justification.\n",
    "- La stratégie d'entraînement et de validation.\n",
    "- Les métriques d’évaluation pertinentes.\n",
    "- Les améliorations possibles (tuning, sélection de variables, etc.).\n",
    "- Une conclusion synthétique.\n",
    "\n",
    "Rédige le tout de façon structurée, claire et professionnelle.\n",
    "Générez le **contenu complet du rapport** en suivant cette structure. Formatez le texte de manière à ce qu’il puisse facilement être converti en **PDF** à l’aide d’un outil markdown-to-pdf ou HTML-to-PDF.\n",
    "\n",
    "Format d’entrée :\n",
    "\n",
    "---\n",
    "Contexte (dataset) :  \n",
    "{context}\n",
    "\n",
    "Question utilisateur :  \n",
    "{question}\n",
    "---\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "    reponse = groq_llm.invoke(prompt.format(context=context,question=query))\n",
    "    return reponse.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_response (query) :\n",
    "    hypothetical_doc = hypothetical_document_generation(query)\n",
    "    relevant_documents = get_relevant_documents(hypothetical_doc,query)\n",
    "    print(\"Number of documents before cleaning (just combination) : \",len(relevant_documents))\n",
    "    clean_documents = clean_retrieved_docs(relevant_documents,query)\n",
    "    print(\"Number of documents after LLM as judge scoring, and filter  : \",len(clean_documents))\n",
    "\n",
    "    context = \"\\n\".join([doc for doc in clean_documents])\n",
    "    \n",
    "    if clean_documents :\n",
    "        response  = llm_report(query,context)\n",
    "    else:\n",
    "        response = llm_report(query,hypothetical_doc)\n",
    "        \n",
    "    return hypothetical_doc,clean_documents,response\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents before cleaning (just combination) :  3\n",
      "20 <class 'int'>\n",
      "98 <class 'int'>\n",
      "0 <class 'int'>\n",
      "Number of documents after LLM as judge cleaning  :  1\n",
      "Churn des clients ( télécommunication)\n",
      "Ce use case a pour objectif de prédire les clients susceptibles de partir afin d’anticiper la\n",
      "majorité des départs des clients et ainsi le département marketing peut mener des actions\n",
      "sur cette population pour la fidéliser\n",
      "* Identifiant de la ligne (numéro de téléphone)\n",
      "* Ancienneté de la ligne en jours (date courante - date d'activation de la ligne)\n",
      "* Date d'activation de la ligne\n",
      "* Date de résiliation de la ligne\n",
      "* Age du client (en année)\n",
      "* Catégorie socio-professionnelle\n",
      "* Ville\n",
      "* Sexe\n",
      "* Etat matrimonial\n",
      "* Type identification (registre du commerce)\n",
      "* Rapport entre la durée des appels sortants et la durée des appels entrant durant les 6 derniers\n",
      "mois\n",
      "* Durée totale des appels (Entrant + Sortant) durant les 6 derniers mois\n",
      "* Part des appels (Entrant + Sortant) réalisés le week-end parmi la totalité des appels (Entrant +\n",
      "Sortant)\n",
      "* Part des appels (Entrant + Sortant) réalisés le soir (20h à 8h) parmi la totalité des appels\n",
      "(Entrant + Sortant)\n",
      "* Nombre total de SMS internationaux envoyés durant les 6 derniers mois\n",
      "* Nombre total de SMS nationaux envoyés vers mobiles MEDITEL durant les 6 derniers mois\n",
      "* Durée des appels sortants réalisés le week-end\n",
      "* Durée moyenne des appels vocaux sortants lors du mois M-1, M-2, M-3, M-4, M-5, M-6\n",
      "* Nombre et durée des appels sortants lors du mois M-1, M-2, M-3, M-4, M-5, M-6\n",
      "* Nombre et durée des appels nationaux, et internationaux, entrants et sortants lors du\n",
      "mois M-1,M-2,M-3,M-4,M-5, M-6\n",
      "* Plainte la plus fréquente lors des 6 derniers mois (Si plusieurs plaintes ont la même\n",
      "* Nombre total de plaintes durant le dernier mois\n",
      "* Nombre de jours écoulés depuis la dernière plainte\n"
     ]
    }
   ],
   "source": [
    "input = \"Comment predire le churn des client en telecomunications\"\n",
    "\n",
    "hypothetical_document,clean_docs,response = final_response(input)\n",
    "\n",
    "print(clean_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Cahier des Charges : Prédiction du Churn des Clients en Télécommunications**\n",
      "\n",
      "**Reformulation du Problème et Objectif**\n",
      "\n",
      "Le problème consiste à prédire les clients susceptibles de partir (churn) dans le secteur des télécommunications, afin d'anticiper la majorité des départs des clients et de permettre au département marketing de mener des actions pour fidéliser cette population.\n",
      "\n",
      "**Description du Dataset**\n",
      "\n",
      "Le dataset comprend 24 variables, dont 2 variables cibles (Date de résiliation de la ligne et Churn) et 22 variables explicatives. Les variables explicatives sont de type numérique (Ancienneté de la ligne, Age du client, Durée des appels, Nombre de SMS, etc.) et catégorielles (Catégorie socio-professionnelle, Ville, Sexe, Etat matrimonial, etc.). Les variables cibles sont binaires (0 pour les clients qui ne partent pas, 1 pour les clients qui partent).\n",
      "\n",
      "**Type de Problème**\n",
      "\n",
      "Le problème est de type classification binaire, où l'objectif est de prédire la probabilité qu'un client parte ou non.\n",
      "\n",
      "**Étapes de Préparation des Données**\n",
      "\n",
      "1. **Nettoyage des données** : vérification des données manquantes, traitement des valeurs aberrantes et des outliers.\n",
      "2. **Transformation des variables** : transformation des variables catégorielles en variables numériques à l'aide de la méthode one-hot encoding.\n",
      "3. **Normalisation des données** : normalisation des variables numériques pour éviter les problèmes de scaling.\n",
      "\n",
      "**Modèles Recommandés**\n",
      "\n",
      "Deux modèles seront testés pour résoudre ce problème :\n",
      "\n",
      "1. **Random Forest** : ce modèle est particulièrement efficace pour les problèmes de classification binaire et permet de gérer les interactions entre les variables explicatives.\n",
      "2. **Gradient Boosting** : ce modèle est également efficace pour les problèmes de classification binaire et permet de gérer les interactions entre les variables explicatives.\n",
      "\n",
      "**Stratégie d'Entraînement et de Validation**\n",
      "\n",
      "1. **Séparation des données** : les données seront séparées en deux parties, une pour l'entraînement (80% des données) et une pour la validation (20% des données).\n",
      "2. **Entraînement des modèles** : les modèles seront entraînés sur les données d'entraînement.\n",
      "3. **Validation des modèles** : les modèles seront évalués sur les données de validation.\n",
      "\n",
      "**Métriques d'Évaluation**\n",
      "\n",
      "Les métriques d'évaluation suivantes seront utilisées pour évaluer les performances des modèles :\n",
      "\n",
      "1. **Accuracy** : mesure de la précision du modèle.\n",
      "2. **Precision** : mesure de la précision du modèle pour les clients qui partent.\n",
      "3. **Recall** : mesure de la sensibilité du modèle pour les clients qui partent.\n",
      "4. **F1-score** : mesure de la performance globale du modèle.\n",
      "5. **AUC-ROC** : mesure de la capacité du modèle à séparer les clients qui partent de ceux qui ne partent pas.\n",
      "\n",
      "**Améliorations Possibles**\n",
      "\n",
      "1. **Tuning des hyperparamètres** : ajustement des hyperparamètres des modèles pour améliorer leurs performances.\n",
      "2. **Sélection de variables** : sélection des variables les plus pertinentes pour améliorer les performances des modèles.\n",
      "3. **Ensemble learning** : combinaison de plusieurs modèles pour améliorer les performances.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Ce cahier des charges propose une approche machine learning pour prédire le churn des clients en télécommunications. Les modèles Random Forest et Gradient Boosting seront testés et évalués en fonction de leurs performances sur les données de validation. Les améliorations possibles seront explorées pour améliorer les performances des modèles.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------QUERY--------------\n",
    "query = \"quelles variables utiliser pour predire le salaire ?\"\n",
    "\n",
    "# retrieved_docs = retriever.invoke(query,k=1)\n",
    "retrieved_docs = vector_store.similarity_search(query, k=1)\n",
    "\n",
    "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "# prompt = f\"Contexte:\\n{context}\\n\\nQuestion: {query}\\rReponse:\"\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction de Salaire\n",
      "* Âge : l'âge de chaque employé, en années.\n",
      "* Genre : le genre de chaque employé, qui peut être soit homme, soit femme.\n",
      "* Niveau d'études : le niveau d'études de chaque employé, qui peut être : lycée,\n",
      "licence (bachelor), master ou doctorat (PhD).\n",
      "* Intitulé du poste : le poste occupé par chaque employé. Les intitulés peuvent\n",
      "varier selon l'entreprise, par exemple : manager, analyste, ingénieur ou\n",
      "administrateur.\n",
      "* Années d'expérience : le nombre d’années d’expérience professionnelle de chaque\n",
      "employé.\n",
      "* Salaire : le salaire annuel de chaque employé en dollars américains. Les valeurs\n",
      "peuvent varier en fonction de facteurs comme l’intitulé du poste, les années\n",
      "d’expérience et le niveau d’études.\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Tu es un assistant expert en machine learning.\n",
    "\n",
    "Voici un contexte contenant des descriptions de variables extraites d’un dataset :\n",
    "\n",
    "{context}\n",
    "\n",
    "Ta tâche est de répondre à la question suivante **en te basant sur les variables présentes dans ce contexte** :\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "Réponse :\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "en te basant uniquement sur ce contexte :\n",
    "\n",
    "{context}\n",
    "\n",
    "répondez à la question suivante :\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "Réponse :\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt1 = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Pour prévoir le salaire, il est important de considérer les facteurs qui ont le plus d'influence sur la rémunération. Selon le contexte fourni, voici les variables que je recommanderais de utiliser pour prévoir le salaire :\n",
      "\n",
      "1. **Âge** : L'âge peut avoir une influence sur la rémunération, car les personnes âgées peuvent avoir plus d'expérience et être considérées comme plus compétentes dans leur domaine.\n",
      "2. **Niveau d'études** : Le niveau d'études est un facteur clé pour déterminer les compétences et les connaissances acquises par un individu, ce qui peut avoir une influence sur la rémunération.\n",
      "3. **Intitulé du poste** : L'intitulé du poste est le plus directement lié au salaire, car il reflète la responsabilité et l'importance de l'emploi.\n",
      "4. **Années d'expérience** : Les années d'expérience sont un facteur clé pour évaluer les compétences et la productivité d'un individu, ce qui peut avoir une influence sur la rémunération.\n",
      "\n",
      "Il est possible de considérer également d'autres variables qui peuvent avoir une influence indirecte sur le salaire, telles que :\n",
      "\n",
      "* **Genre** : Bien qu'il ne s'agisse pas d'une variable directement liée au salaire, il est important de prendre en compte les discriminances salariales pour garantir l'équité dans la rémunération.\n",
      "* **Autres facteurs non explicités** : Il est possible que d'autres facteurs, tels que la santé ou les compétences sociales, aient une influence sur le salaire.\n",
      "\n",
      "Il conviendrait de recourir à des techniques de prédiction comme l'apprentissage automatique (ML) pour identifier les variables les plus importantes et développer une modélisation capable de prévoir avec précision les salaires en fonction de ces facteurs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm=OllamaLLM(model='llama3.2:latest')\n",
    "response = llm.invoke(rag_prompt.format(context=context, question=query))\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(question, context, answer):\n",
    "    prompt = f\"\"\"Vous allez évaluer la réponse d'un assistant virtuel à une question basée sur un contexte donné.\n",
    "\n",
    "    Question: {question}\n",
    "    Contextw: {context}\n",
    "    Reponse: {answer}\n",
    "\n",
    "    Cette réponse est elle juste pour le context donné ? Répond par :\n",
    "    - Correcte\n",
    "    - Incorrecte\n",
    "    Et expliquez bievement pourquoi .\n",
    "    \n",
    "    Si le contexte ne contient pas d'informations pertinentes pour répondre à la question, répondez par \"Contexte non correspondant\".\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt)\n",
    "     \n",
    "\n",
    "\n",
    "def correct_answer(question, context):\n",
    "    prompt = f\"\"\"La réponse précédente était incorrecte. Veuillez générer une réponse correcte en vous basant strictement sur le contexte.\n",
    "\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Réponse correcte: \n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(rag_prompt.format(context=context, question=query))\n",
    "\n",
    "evaluation = evaluate_answer(query, context, response)\n",
    "\n",
    "\n",
    "i=0\n",
    "while \"Correcte\" not in evaluation:\n",
    "    i=i+1\n",
    "    print(\"Evaluation:\", evaluation)\n",
    "    if \"Incorrecte\" in evaluation:\n",
    "        response = correct_answer(query, context)\n",
    "        # print(\"Réponse corrigée:\", response)\n",
    "        evaluation = evaluate_answer(query, context, response)\n",
    "    elif  \"Contexte non correspondant\" in evaluation:\n",
    "      query = input(\"S'il vous plaît, reformulez la question : \")\n",
    "      retrieved_docs = vector_store.similarity_search(query, k=1)\n",
    "      context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "      response = llm.invoke(rag_prompt.format(context=context, question=query))\n",
    "      evaluation = evaluate_answer(query, context, response)\n",
    "    else:\n",
    "        print(\"Erreur d'évaluation, veuillez vérifier le modèle.\")\n",
    "        break\n",
    "\n",
    "# Sortie de la boucle sigifie que la réponse est correcte\n",
    "if i>0:\n",
    "    print(f\"Réponse Regénerée {i} fois : \", response)\n",
    "else:\n",
    "    print(\"Réponse initiale correcte:\", response)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[SystemMessage(content=\"Current summary:\\nMarouan introduces himself as a data scientist and expresses his love for sushi.\\n\\nNew lines of conversation:\\nAI: Nice to meet you, Marouan. I'm delighted to learn that you're a data scientist. Did you know that the field of data science has its roots in the 1960s, when computer scientists like John Tukey and Peter Naur began exploring the intersection of statistics and computer programming? As for sushi, I'm familiar with the various types of sushi, including maki, nigiri, sashimi, and temaki. Which type of sushi is your favorite, Marouan?\\n\\nNew summary:\\nMarouan introduces himself as a data scientist and expresses his love for sushi. The AI is familiar with the history of data science and various types of sushi, and asks Marouan about his favorite type of sushi.\", additional_kwargs={}, response_metadata={})]\n",
      "Human: what was my name\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Nice to recall that detail, Marouan. Your name is Marouan, and you've introduced yourself as a data scientist with a passion for sushi.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Step 1: Set up the LLM\n",
    "\n",
    "# Step 2: Add a memory object\n",
    "# memory = ConversationSummaryMemory(llm=groq_llm,return_messages=True)\n",
    "\n",
    "# # Step 3: Create a conversational chain\n",
    "# conversation = ConversationChain(\n",
    "#     llm=groq_llm,\n",
    "#     memory=memory,\n",
    "#     verbose=True  # shows what's going into the prompt\n",
    "\n",
    "# )\n",
    "\n",
    "# Step 4: Interact\n",
    "# print(conversation.predict(input=\"Hi, my name is Marouan.\"))\n",
    "# print(conversation.predict(input=\"What is my name?\"))\n",
    "# print(conversation.predict(input=\"Remember that I love sushi.\"))\n",
    "print(conversation.predict(input=\"what was my name\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm a large language model, I don't have the ability to retain information about previous conversations or keep track of individual users. Each time you interact with me, it's a new conversation and I don't have any prior knowledge about you. If you'd like to share your name with me, I'd be happy to chat with you!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 39, 'total_tokens': 109, 'completion_time': 0.093333333, 'prompt_time': 0.002431146, 'queue_time': 0.124832525, 'total_time': 0.095764479}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_a4265e44d5', 'finish_reason': 'stop', 'logprobs': None} id='run-73ddbdd1-e85b-4a1f-bfbc-19a84bcd5734-0' usage_metadata={'input_tokens': 39, 'output_tokens': 70, 'total_tokens': 109}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "\n",
    "# Prompt moderne\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "])\n",
    "\n",
    "# Mémoire\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "\n",
    "history_store = {}\n",
    "\n",
    "# Fonction pour retourner une instance de message history par session\n",
    "def get_message_history(session_id: str):\n",
    "    if session_id not in history_store:\n",
    "        history_store[session_id] = ChatMessageHistory()\n",
    "    return history_store[session_id]\n",
    "    \n",
    "# Runnable avec historique\n",
    "chain = RunnableWithMessageHistory(\n",
    "    prompt | groq_llm,\n",
    "    get_session_history=get_message_history,\n",
    "    input_messages_key=\"text\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "# Appel de la chaîne (simulateur de session)\n",
    "# result1 = chain.invoke({\"text\": \"i like football , its my favorite\"}, config={\"configurable\": {\"session_id\": \"user1\"}})\n",
    "# print(result1)\n",
    "\n",
    "result2 = chain.invoke({\"text\": \"what was my name\"}, config={\"configurable\": {\"session_id\": \"user1\"}})\n",
    "print(result2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.messages.BaseChatMessageHistory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[139]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate, MessagesPlaceholder\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhistory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnableWithMessageHistory\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseChatMessageHistory, InMemoryHistory\n\u001b[32m      8\u001b[39m store = {}\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_by_session_id\u001b[39m(session_id: \u001b[38;5;28mstr\u001b[39m) -> BaseChatMessageHistory:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\Desktop\\ML_RAG\\venv\\Lib\\site-packages\\langchain_core\\messages\\__init__.py:142\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr_name)\u001b[39m\n\u001b[32m    140\u001b[39m package = __spec__.parent\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_name == \u001b[33m\"\u001b[39m\u001b[33m__module__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m module_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     result = \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mattr_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    144\u001b[39m     module = import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, package=package)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     88\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_core.messages.BaseChatMessageHistory'"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import BaseChatMessageHistory, InMemoryHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're an assistant who's good at {ability}\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | ChatAnthropic(model=\"claude-2\")\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    # Uses the get_by_session_id function defined in the example\n",
    "    # above.\n",
    "    get_by_session_id,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "print(chain_with_history.invoke(  # noqa: T201\n",
    "    {\"ability\": \"math\", \"question\": \"What does cosine mean?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"foo\"}}\n",
    "))\n",
    "\n",
    "# Uses the store defined in the example above.\n",
    "print(store)  # noqa: T201\n",
    "\n",
    "print(chain_with_history.invoke(  # noqa: T201\n",
    "    {\"ability\": \"math\", \"question\": \"What's its inverse\"},\n",
    "    config={\"configurable\": {\"session_id\": \"foo\"}}\n",
    "))\n",
    "\n",
    "print(store)  # noqa: T201"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
